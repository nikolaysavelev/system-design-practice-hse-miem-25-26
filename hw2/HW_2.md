# Домашнее задание №2
## Контекст
Вы, наверное, слышали про Apache Kafka.

Kafka является распределенным программным брокером (*). Kafka можно использовать в асинхронных интеграциях, можно использовать для потоковой обработки данных в реальном времени. Основная фишка - высокая пропускная способность (throughput) и низкая задержка (latency).

В этом курсе мы лишь немного почувствуем throughput и latency (вы должны в этой домашке запустить небольшое НТ и рассказать о результатах).

Остановимся больше на масштабируемости и отказоустойчивости: Kafka работает в кластерном режиме.

(*) Брокер сообщений - это программное обеспечение, которое выступает посредником для обмена данными между различными приложениями или системами. Он принимает сообщения от одного приложения, обрабатывает их и доставляет другому, гарантируя надежную доставку, даже если получатель временно недоступен. Основные функции брокера включают хранение сообщений в очередях или темах, маршрутизацию, преобразование формата сообщений и обеспечение асинхронного обмена данными.

## Пару слов: как работает Kafka
Представьте Kafka не как "очередь" (где сообщение исчезает после прочтения, как например в RabbitMQ, хотя там тоже можно сделать топики), а как распределенный журнал, а ля текстовые файлы, которым можно писать в конец и эти файлы можно раскинуть по разным хостам.

- Event (событие)

    Событие (event) фиксирует факт, что что-то произошло IRL. В документации это также называется записью (record) или сообщением (message). Когда вы читаете или записываете данные в Kafka, вы делаете это в форме событий. Концептуально событие имеет ключ, значение, отметку времени и необязательные заголовки метаданных. Пример события:
    ```json
    Event key: "Alice" 
    Event value: "Made a payment of $200 to Bob" 
    Event timestamp: "Jun. 25, 2020 at 2:06 p.m." 
    ```

- Producers / Consumers (производители / потребители)

    Производители (Producers) - это те клиентские приложения, которые публикуют (записывают) события в Kafka, а потребители (consumers) - это те, которые подписываются на эти события (читают и обрабатывают). В Kafka производители и потребители полностью отделены друг от друга и не зависят друг от друга, что является ключевым элементом дизайна для достижения высокой масштабируемости, которой известна Kafka. Например, производителям никогда не нужно ждать потребителей. Kafka предоставляет различные гарантии, например возможность обрабатывать события ровно один раз.

- Topics (тема)

    События организованы и надежно хранятся в темах (topics). Очень упрощенно, тема похожа на папку в файловой системе, а события - это файлы в этой папке. Примером названия темы может быть «платежи». Темы в Kafka всегда имеют несколько производителей и подписчиков: у темы может быть ноль, один или несколько производителей, которые записывают в нее события, а также ноль, один или несколько потребителей, которые подписываются на эти события. События в теме можно читать сколь угодно часто - в отличие от традиционных систем обмена сообщениями, события не удаляются после использования. Вместо этого вы определяете, как долго Kafka должна хранить ваши события с помощью настройки конфигурации для каждой темы, после чего старые события будут отброшены. Производительность Kafka практически не зависит от размера данных, поэтому хранить данные в течение длительного времени - это нормально.

- Buckets and Partitions (корзины и партиции)

    Темы разделены, то есть тема распределена по нескольким корзинам (buckets), расположенным на разных брокерах Kafka. Это распределенное размещение ваших данных очень важно для масштабируемости, поскольку оно позволяет клиентским приложениям одновременно читать и записывать данные от многих брокеров и на них. Когда новое событие публикуется в теме, оно фактически добавляется к одному из разделов (partition) темы. События с одним и тем же ключом события (например, идентификатор клиента или транспортного средства) записываются в один и тот же раздел, и Kafka гарантирует, что любой потребитель данного раздела темы всегда будет читать события этого раздела в том же порядке, в котором они были записаны.

- Replication (кратко)

    Чтобы сделать данные отказоустойчивыми и высокодоступными, каждую тему можно реплицировать даже в географических регионах или центрах обработки данных, поэтому всегда есть несколько брокеров, у которых есть копия данных на случай, если что-то пойдет не так, а вы хотите обслуживание брокеров и т. д. Обычной производственной настройкой является коэффициент репликации 3, то есть всегда будет три копии ваших данных. Эта репликация выполняется на уровне разделов тем (topic-partitions).

- Replication (полно)

    https://habr.com/ru/companies/otus/articles/790504/

## В двух словах: как работает Raft (и почему нет Zookeeper)

Раньше Kafka использовала отдельную систему (которую я показывал на семинаре) - Zookeeper (distributed coordination system). Кому интересно, прошу еще раз прочитать статью, какие функции выполняют такие системы: https://www.geeksforgeeks.org/system-design/distributed-coordination-based-systems/

Если кратко, они нужны, чтобы хранить метаданные (кто жив из брокеров, кто лидер, где какие топики). Это было неудобно (нужно админить две системы). Современная Kafka (начиная с версии 3.3+) работает в режиме KRaft (Kafka Raft).

Суть Raft: брокеры сами выбирают главного (в терминологии кафки Контроллера) путем голосования. Чтобы решение было принято (например, "Брокер-1 умер, теперь Лидером является Брокер-2"), за него должно проголосовать большинство (кворум).

Для кластера из 3 узлов кворум - это 2. Если останется 1 живой узел из 3, кластер помрет (т.к. 1 не является большинством). Это цена за гарантию целостности данных (Consistency over Availability).

Если интересно, можно посмотреть документацию YaCloud по Kafka с [KRaft](https://yandex.cloud/ru/docs/managed-kafka/operations/cluster-create?utm_referrer=https%3A%2F%2Fwww.google.com%2F#:~:text=%D0%B2%D1%8B%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%BD%D1%8B%D1%85%20%D1%85%D0%BE%D1%81%D1%82%D0%B0%20ZooKeeper.-,%D0%A5%D0%BE%D1%81%D1%82%D1%8B%20%D1%81%20KRaft,-%D0%9F%D1%80%D0%B8%20%D0%B2%D1%8B%D0%B1%D0%BE%D1%80%D0%B5%20%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D0%B8).

## Суть домашки
Запускаем кластер из 3-х брокеров в Docker и развлекаемся с ним.

Ремарка. Да, в Production-ready решениях очень странно запускать 3 брокера на 1 хосте и ожидать отказоустойчивости (как в Docker, собственно). Поэтому большинство реальных кластерных кафок в реальных проектах обычно исполнены либо в k8s, либо один брокер на одну машину (физическую или виртуальную).
Мы во имя простоты делаем в докере + это может быть удобно разработчикам, у которых нет возможности в dev-окружении развернуть все не локально.

- **Шаг 0: Подготовка**

    В вашем репозитории с домашками добавьте файл hw2/HW_2_SOLUTION.md

- **Шаг 1: Склонируйте этот репозиторий локально**
    ```bash
    git clone <this-repo>
    ```

    Вы увидите kafka-kraft-cluster-docker-compose (спасибо [этому ресурсу](https://github.com/minhhungit/kafka-kraft-cluster-docker-compose) за подготовку репозитория, я его подрезал и немного видоизменил).

- **Шаг 2: Запустите**
    ```bash
    docker compose up -d
    ```

    Итого, у вас получился кластер с 3 нодами кафки (kafka01, kafka02, kafka03).

    Создадим топик с помощью команды:
    ```bash
    docker run -it --rm --network kafka-kraft-cluster_default confluentinc/cp-kafka:latest kafka-topics \
    --create \
    --topic test_topic \
    --bootstrap-server kafka01:9092,kafka02:9092,kafka03:9092 \
    --partitions 8 \
    --replication-factor 3 \
    --if-not-exists
    ```

    Чтобы убедиться, что кластер жив и здоров, перейдите на http://localhost:8080/ui/clusters/local/brokers

    Должно быть что-то вроде

    ![Kafka cluster](img/image.png)

    Мы видим в первом столбце, какая из нод является контроллером. Контроллеры — это специальные брокеры, которые отвечают за управление кластером и координацию работы других брокеров. Контроллер отслеживает состояние кластера, назначает лидеров для партиций и управляет репликами.

    Если перейдем в раздел Topics, будет тестовый топик test_topic, у которого Replication factor = 3.

    ![Topics](img/topics.png)

- **Шаг 3: А точно правильно настроили?**

    В отчете отразите основные характеристики топика и зачем они нужны. Например, почему здесь столько партиций? А что если их не будет? Насколько отказоустойчивый наш топик?

    ![Topic](img/our_topic.png)

- **Шаг 4: НТ**

    Запустите в терминале эту команду

    ```bash
    docker run -it --rm --network kafka-kraft-cluster_default confluentinc/cp-kafka /bin/kafka-producer-perf-test --topic test_topic --num-records 1000000 --throughput -1 --producer-props bootstrap.servers=kafka01:9092,kafka02:9092,kafka03:9092 batch.size=16384 acks=1 linger.ms=50 --record-size 1000
    ```

    Здесь мы бросаем 1 млн событий батчами. Сделайте скриншот результатов из терминала, сделайте скриншот Kafka UI, что события дошли. Объясните, что к чему в результатах. Сколько RPS? Сколько Latency? На всех ли брокерах есть события?

- **Шаг 5: Отказоустойчивость**

    Сделаем три теста: 

        * убийство контроллера, 
        * убийство 2/3, 
        * восстановление.

    Выполните команду (или pip)
    ```bash
    pip3 install kafka-python
    ```

    Запустите скрипт (или просто python): он кидает события каждые 0.5 секунд в топик `critical_data`.
    ```bash
    python3 example.py
    ```
    Убедитесь в Kafka UI, что события летят.

    С помощью команды docker stop или через Docker Desktop остановите контейнер с брокером, который является Контроллером. 

        * Проверьте в Kafka UI состояние кластера и брокеров. Сделайте скриншот и опишите, как изменились показатели.

        * Убедитесь, что данные все еще летят в топик и что отказ одного из брокеров не повлиял на работоспособность нашего скрипта.

    Остановите следующий брокер, который является контроллером. 

        * Что стало с программой? Продолжаем ли мы писать в топик данные? Если нет, почему это произошло. Что такое ISR (In-Sync Replicas)?

        * Заскриньте логи контейнера последнего живого брокера и объясните значение.

        * Проверьте работоспособность кластера, Kafka UI, kafka kouncil. Что-то работает?

     Восстановите один из контейнеров, например, который убивали первым.

        * Продолжила ли выполняться ваша программа, исчезла ли ошибка? Заскриньте Kafka UI и опишите, что сейчас происходит с кластером, он жив?

    Восстановите последний контейнер.

        * Все ли хорошо с кластером? Везде ли данные консистентны? 

## Как сдавать
1. Пришлите в телеграм @nikolaysavelev ссылку на ваш репозиторий
2. В репозитории должен быть проект, в директории hw2 должен быть файл HW_2_SOLUTION.md. Формат отчета свободный.

## Критерии оценки
1. Если смогли запустить и пострелять - уже отлично!
2. Если смогли сделать НТ - вообще супер!
3. Если смогли проанализировать и понять как работает кластер, поиграться с отказоустойчивостью - спасибо большое, высший балл!